Automatically generated by Mendeley Desktop 1.17.9
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Preferences -> BibTeX in Mendeley Desktop

@article{Beal2006,
author = {Beal, M J and Ghahramani, Z},
doi = {DOI:10.1214/06-BA126},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Beal, Ghahramani/Bayesian Analysis/Beal, Ghahramani - 2006 - Variational Bayesian Learning of Directed Graphical Models with Hidden Variables.pdf:pdf},
issn = {1936-0975},
journal = {Bayesian Analysis},
keywords = {approximate bayesian inference,bayes factors,directed acyclic,em algorithm,graphical models,graphs,markov chain monte carlo,model,selection,variational bayes},
number = {4},
pages = {793--832},
title = {{Variational Bayesian Learning of Directed Graphical Models with Hidden Variables}},
volume = {1},
year = {2006}
}
@book{Bishop2006,
author = {Bishop, Christopher M.},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Bishop/Unknown/Bishop - 2006 - Pattern Recognition and Machine Learning.pdf:pdf},
publisher = {Springer},
title = {{Pattern Recognition and Machine Learning}},
year = {2006}
}
@article{Blei2016,
abstract = {One of the core problems of modern statistics is to approximate difficult-to-compute probability distributions. This problem is especially important in Bayesian statistics, which frames all inference about unknown quantities as a calculation about the posterior. In this paper, we review variational inference (VI), a method from machine learning that approximates probability distributions through optimization. VI has been used in myriad applications and tends to be faster than classical methods, such as Markov chain Monte Carlo sampling. The idea behind VI is to first posit a family of distributions and then to find the member of that family which is close to the target. Closeness is measured by Kullback-Leibler divergence. We review the ideas behind mean-field variational inference, discuss the special case of VI applied to exponential family models, present a full example with a Bayesian mixture of Gaussians, and derive a variant that uses stochastic optimization to scale up to massive data. We discuss modern research in VI and highlight important open problems. VI is powerful, but it is not yet well understood. Our hope in writing this paper is to catalyze statistical research on this widely-used class of algorithms.},
archivePrefix = {arXiv},
arxivId = {1601.00670},
author = {Blei, David M. and Kucukelbir, Alp and McAuliffe, Jon D.},
eprint = {1601.00670},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Blei, Kucukelbir, McAuliffe/Unknown/Blei, Kucukelbir, McAuliffe - 2016 - Variational Inference A Review for Statisticians.pdf:pdf},
keywords = {Graphical Model,Variational Inference},
title = {{Variational Inference: A Review for Statisticians}},
year = {2016}
}
@book{Murphy1991,
abstract = {Some of the most remarkable issues related to interharmonics observed from a probabilistic perspective are presented. Attention is firstly devoted to interharmonic frequency and amplitude variability. Starting from the basic mathematical and computational aspects of probabilistic harmonic models, the difficulties to include interharmonics are discussed with particular attention to the problem of the frequency resolution and of the computational burden. Then, simulation and measurement aspects are discussed, also showing some numerical and experimental results.},
author = {Murphy, Kevin P.},
booktitle = {Machine Learning: A Probabilistic Perspective},
doi = {10.1007/SpringerReference_35834},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Murphy/Machine Learning A Probabilistic Perspective/Murphy - 1991 - Machine Learning A Probabilistic Perspective.pdf:pdf},
publisher = {The MIT Press},
title = {{Machine Learning: A Probabilistic Perspective}},
year = {1991}
}
@article{Kass1995,
author = {Kass, Robert and Raftery, Adrian},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Kass, Raftery/Journal of the American Statistical Association/Kass, Raftery - 1995 - Bayes Factors.pdf:pdf},
journal = {Journal of the American Statistical Association},
number = {430},
title = {{Bayes Factors}},
volume = {90},
year = {1995}
}
@manual{Jamil2017,
address = {R Package version 0.6.4},
annote = {R package version 0.6.4.9003},
author = {HJ},
publisher = {CRAN/GitHub},
title = {{iprior: Linear Regression using I-Priors}},
year = {2017}
}
@article{Bergsma2017,
author = {Bergsma, Wicher},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Bergsma/Manuscript in preparation/Bergsma - 2017 - Regression with I-priors.pdf:pdf},
journal = {Manuscript in preparation},
title = {{Regression with I-priors}},
year = {2017}
}
