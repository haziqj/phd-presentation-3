\subsection{The latent variable motivation}

\begin{frame}{The latent variable motivation}
  \begin{itemize}
    \item<1-> Consider binary responses $y_1, \dots, y_n$ together with their corresponding covariates $x_1, \dots, x_n$. 
    \item<1-> For $i=1,\dots,n$, model the responses as
    \[
      y_i \sim \Bern(p_i).
    \]
    \item<2-> Assume that there exists continuous, underlying latent variables $y_1^*, \dots, y_n^*$, such that
    \[
      y_i =
      \begin{cases}
        1 & \text{ if } y_i^* \geq 0 \\
        0 & \text{ if } y_i^* < 0.    \\
      \end{cases}
    \]
    \item<3-> Model these continuous latent variables according to
    \[
      y_i^* = f(x_i) + \epsilon_i
    \]
    where $(\epsilon_1, \dots, \epsilon_n) \sim \N(\bzero, \bPsi^{-1})$ and $f \in \cF$ (some RKHS).
  \end{itemize}
\end{frame}

\subsection{Using I-priors}
\begin{frame}{Using I-priors}
  \vspace{-5pt}
  \begin{itemize}\setlength\itemsep{0.5em}
    \only<1|handout:0>{
    \item Assume an I-prior on $f$. Then,
    \begin{align*}
      \begin{gathered}
        f(x_i) = {\color{white} \overbrace{\color{black} f_0(x_i)}^{\alpha}} 
        + \sum_{k=1}^n h_\lambda(x_i, x_k)w_k \\
        (w_1, \dots, w_n) \sim \N(\bzero, \bPsi) \\
      \end{gathered}
    \end{align*}
    }
    \only<2->{
    \item Assume an I-prior on $f$. Then,
    \begin{align*}
      \begin{gathered}
        f(x_i) = {\color{gray} \overbrace{\color{black} f_0(x_i)}^{\alpha}} 
        + \sum_{k=1}^n h_\lambda(x_i, x_k)w_k \\
        (w_1, \dots, w_n) \sim \N(\bzero, \bPsi). \\
      \end{gathered}
    \end{align*}
    }
    \item<2-> For now, consider iid errors $\bPsi = \psi\bI_n$. \onslide<3->{In this case,
    \begin{align*}
      p_i = \Prob[y_i = 1] &= \Prob[y_i^* \geq 0] \\
      &= \Prob[\epsilon_i \leq f(x_i)] \\
      &= \Phi\Big(\psi^{1/2} ( 
%      {\color{gray} 
%      \underbrace{{\color{black} \alpha + {\textstyle\sum_{k=1}^n} h_\lambda(x_i, x_k)w_k}}_{\eta_i}
%      }
      \alpha + {\textstyle\sum_{k=1}^n} h_\lambda(x_i, x_k)w_k
      ) \Big)
    \end{align*}
    where $\Phi$ is the CDF of a standard normal.}
    \item<4-> No loss of generality compared with using an arbitrary threshold $\tau$ or error precision $\psi$. Thus, set $\psi = 1$.
  \end{itemize}
\end{frame}

\subsection{Estimation (and challenges)}

\begin{frame}[label=estimation]{Estimation}
  \vspace{5pt}
  \begin{columns}
    \uncover<1->{
    \begin{column}{0.47\textwidth}
      \vspace{6pt}
      \begin{itemize}\setlength\itemsep{0.5em}
        \item Denote $f_i = f(x_i)$ for short.
        \item The marginal density
        \vspace{4pt}
        \[
          \hspace{0.75cm} p(\by) = \int p(\by | \bff) p(\bff) \d\bff 
        \]
      \end{itemize}
    \end{column}}
    \uncover<4->{
    \begin{column}{0.5\textwidth}
    \vspace{-42pt}
      \begin{center}
        \includegraphics[scale=0.40]{figure/taylor_expand_meme}
      \end{center}
    \end{column}}
  \end{columns}
  \uncover<1->{
  \vspace{-5pt}
  \[
    \phantom{p(\by)} = \int \prod_{i=1}^n \left[ \Phi(f_i)^{y_i} \big(1 - \Phi(f_i)\big)^{1-y_i} \right] \cdot \N(\alpha\bone_n, \bH_\lambda^2) \d\bff
  \]
  \hspace{0.65cm} for which $p(\bff|\by)$ depends, cannot be evaluated analytically.}
  \vspace{3pt}
  \begin{itemize}
    \item<2-> Some strategies:
    \begin{itemize}
      \item[\xmark]<2-> Naive Monte-Carlo integral
      \item[\xmark]<3-> EM algorithm with a MCMC E-step
      \item[{\color{FUorange}\cmark}]<4-> Laplace approximation 
        \raisebox{-1.1pt}{\hyperlink{laplace}{\beamerbutton{details}}}
      \item[{\color{FUorange}\cmark}]<5-> MCMC sampling 
        \raisebox{-1.1pt}{\hyperlink{mcmc}{\beamerbutton{details}}}
    \end{itemize}
  \end{itemize}
\end{frame}

%\begin{frame}
%  \begin{center}
%    \includegraphics[scale=0.5]{figure/taylor_expand_meme}
%  \end{center}
%\end{frame}
