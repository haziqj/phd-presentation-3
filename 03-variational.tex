\subsection{Introduction}

\begin{frame}{Variational inference}
  \vspace{-10pt}
  \begin{itemize}
    \item Name derived from calculus of variations which deals with maximising or minimising functionals.
    \begin{table}
      \begin{tabular}{l  l  l }
      Functions   &$p:\theta \mapsto \bbR$  &(standard calculus) \\ 
      Functionals &$\cH:p \mapsto \bbR$     &(variational calculus) \\ 
      \end{tabular}
    \end{table}
  \item Using standard calculus, we can solve
  \[
    \argmax_\theta p(\theta) =: \hat\theta
  \]
  e.g. $p$ is a likelihood function, and $\hat\theta$ is the ML estimate.
  \item Using variational calculus, we can solve
  \[
    \argmax_p \cH(p) =: \tilde p
  \]
  e.g. $\cH$ is the entropy $\cH = - \int p(x)\log p(x) \d x$, and $\tilde p$ is the entropy maximising distribution.
  \end{itemize}
  \vspace{5pt}
\end{frame}

\begin{frame}{Variational inference (cont.)}
  \blfootnote{\fullcite[Ch. 10]{Bishop2006}}
  \blfootnote{\fullcite[Ch. 21]{Murphy1991}}
  \vspace{-5pt}
  \begin{itemize}\setlength\itemsep{0.8em}
    \item Consider a statistical model where we have observations $(y_1, \dots, y_n)$ and also some latent variables $(z_1, \dots, z_n)$.
    \item The $z_i$ could be random effects or some auxiliary latent variables.
    \item In a Bayesian setting, this could also include the parameters to be estimated.
    \item \textbf{GOAL}: Find approximations for
    \begin{itemize}
      \item The posterior distribution $p(\bz|\by)$; and
      \item The marginal likelihood (or model evidence) $p(\by)$.
    \end{itemize}
    \item Variational inference is a deterministic approach, unlike MCMC.
  \end{itemize}
\end{frame}

\begin{frame}{Decomposition of the log marginal}  
  \vspace{-3pt}
  \begin{itemize}\setlength\itemsep{0.5em}
    \item Let $q(\bz)$ be some density function to approximate $p(\bz|\by)$. Then the log-marginal density can be decomposed into
    \begin{align*}
      \log p(\by) &= \log p(\by,\bz) - \log p(\bz|\by) \\
       &= \int \left\{ \log \frac{p(\by,\bz)}{q(\bz)} - \log \frac{p(\bz|\by)}{q(\bz)} \right\} q(\bz) \d \bz \\    
      &=  \cL(q) +  \KL(q \Vert p) \\
      &\geq \cL(q)    
    \end{align*}
    \item $\cL$ is referred to as the ``lower-bound'', and it serves as a surrogate function to the marginal.
    \item Maximising the $\cL(q)$ is equivalent to minimising $\KL(q \Vert p)$.
    \item Although $\KL(q \Vert p)$ is minimised at $q(\bz) \equiv p(\bz|\by)$ (c.f. EM algorithm), we are unable to work with $p(\bz|\by)$.
  \end{itemize}
\end{frame}

\begin{frame}{Comparison of approximations (density)}
  \vspace{-5pt}
  \only<1>{
    \begin{center}
      \includegraphics[scale=0.7]{figure/compare1}
    \end{center}
  }
  \only<2>{
    \begin{center}
      \includegraphics[scale=0.7]{figure/compare2}
    \end{center}
  }  
  \only<3>{
    \begin{center}
      \includegraphics[scale=0.7]{figure/compare3}
    \end{center}
  } 
  \only<4>{
    \begin{center}
      \includegraphics[scale=0.7]{figure/compare4}
    \end{center}
  } 
  \only<5>{
    \begin{center}
      \includegraphics[scale=0.7]{figure/compare5}
    \end{center}
  } 
\end{frame}

\begin{frame}{Comparison of approximations (deviance)}
  \vspace{-5pt}
  \begin{center}
    \includegraphics[scale=0.7]{figure/compare6}
  \end{center}
\end{frame}

\begin{frame}{Factorised distributions (Mean-field theory)}
  \blfootnote{\fullcite{Blei2016}}
  \vspace{-20pt}
  \begin{itemize}
    \item Maximising $\cL$ over all possible $q$ not feasible. Need some restrictions, but only to achieve tractability.
    \item Suppose we partition elements of $\bz$ into $m$ disjoint groups $\bz = (\bz^{(1)}, \dots, \bz^{(m)})$, and assume
    \[
      q(\bz) = \prod_{j=1}^m q_j(\bz^{(j)})
    \]
    \item Under this restriction, the solution to $\argmax_q \cL(q)$ is
    \begin{align}\label{eq:meanfieldsoln}
      \tilde q_j(\bz^{(j)}) \propto \exp\big(\E_{-j}[\log p(\by,\bz)]\big)
    \end{align}
    for $j \in \{1,\dots,m\}$.
    \item In practice, these unnormalised densities are of recognisable form (especially if conjugate priors are used).
    \vspace{4pt}
  \end{itemize}
\end{frame}

\begin{frame}{Coordinate ascent mean-field variational inference (CAVI)}
  \vspace{-5pt}
  \begin{itemize}\setlength\itemsep{0.4em}
    \item The optimal distributions are coupled with another, i.e. each $\tilde q_j(\bz^{(j)})$ depends on the optimal moments of $\bz^{(k)}$, $k \in \{1,\dots,m:k \neq j\}$.
    \item One way around this to employ an iterative procedure.
    \item Assess convergence by monitoring the lower bound
    \[
      \cL(q) = \E_{ q}[\log p(\by, \bz)] - \E_{ q}[\log q(\bz)].
    \]
  \end{itemize}
  \vspace{-12pt}
    \begin{center}
    \scalebox{0.9}{
    \begin{minipage}{\linewidth}
  \begin{algorithm}[H]
    \caption{CAVI}\label{alg:cavi}
    \begin{algorithmic}[1]
      \State \textbf{initialise} Variational factors $q_j(\bz^{(j)})$
      \While{$\cL(q)$ not converged}
        \For{$j = 1,\dots,m$}
          \State $\log q_j(\bz^{(j)}) \gets \E_{-j}[\log p(\by,\bz)] + \const$
        \EndFor
        \State $\cL(q) \gets \E_{ q}[\log p(\by, \bz)] - \E_{ q}[\log q(\bz)]$
      \EndWhile
      \State \textbf{return} $\tilde q(\bz) = \prod_{j=1}^m \tilde q_j(\bz^{(j)})$
    \end{algorithmic}
  \end{algorithm}
      \end{minipage}%
    }
  \end{center}
\end{frame}

\subsection{A simple example}

\setbeamercovered{still covered={\opaqueness<1->{0}},again covered={\opaqueness<1->{15}}}
\begin{frame}{Estimation of a 1-dim Gaussian mean and variance}
  \vspace{-5pt}
  \begin{itemize}
    \item<1-3> \textbf{GOAL}: Bayesian inference of mean $\mu$ and variance $\psi^{-1}$
    \begin{center}
      {\def\arraystretch{1.2}
      \begin{tabular}{c c l}
        $\displaystyle y_i \iid \N(\mu, \psi^{-1})$ & & Data \\
        {\color{gray!88}$\displaystyle \mu|\psi \sim \N\big(\mu_0,(\kappa_0\psi)^{-1}\big)$} & & {\color{gray!88}\multirow{2}{*}{Priors}} \\
        {\color{gray!88}$\displaystyle \psi \sim \Gamma(a_0,b_0)$} & \\
        $\displaystyle i=1,\dots,n$ & \\
      \end{tabular}
      }
    \end{center}
    \item<2-3> Substitute $p(\mu,\psi|\by)$ with the mean-field approximation
    \[
      q(\mu, \psi) = q_\mu(\mu) q_\psi(\psi)
    \]
    \item<3-> From \eqref{eq:meanfieldsoln}, we can work out the solutions 
    
    \only<5>{
    \begin{align*}
      \log \tilde q_\mu(\mu) 
      &= \E_\psi[\log p(\by|\mu,\psi)] + \E_\psi[\log p(\mu|\psi)] + \const \\[0.5em]
    \log \tilde q_\psi(\psi) 
    &= \E_\mu[\log p(\by|\mu,\psi)] + \E_\mu[\log p(\mu|\psi)] + \log p(\psi) \\
    &\phantom{==} + \const
    \end{align*}
    }
    
    \vspace{-3pt}
    \begin{gather*}
      \uncover<6->{
      \hspace{-12pt}\tilde q_\mu(\mu) \equiv \N \left(\frac{\kappa_0 \mu_0 + n\bar y}{\kappa_0 + n}, \frac{1}{(\kappa_0 + n)\E_q[\psi]} \right)}
      \uncover<7->{
      \ \text{ and } \ \
      \tilde q_\psi(\psi) \equiv \Gamma(\tilde a, \tilde b) \\[0.5em]
      \hspace{-12pt}\tilde a = a_0 + \half[n] \hspace{1.1cm} \tilde b = b_0 + \half \E_q\Big[ {\textstyle\sum_{i=1}^n} (y_i - \mu)^2 + \kappa_0(\mu - \mu_0)^2 \Big]
      }
    \end{gather*}
  \end{itemize}
  
  \only<4->{
  \begin{textblock*}{0.8\textwidth}(1.35cm,0.215\textheight)
    \begin{block}{}
      \begin{itemize}
        \item Under the mean-field restriction, the solution to $\argmax_q \cL(q)$ is
        \begin{align*}
          \tilde q_j(\bz^{(j)}) \propto \exp\big(\E_{-j}[\log p(\by,\bz)]\big)
          \rlap{\hspace{1.3cm}(1)}
        \end{align*}
        for $j \in \{1,\dots,m\}$.  
      \end{itemize}
    \end{block}
  \end{textblock*}
  }

\end{frame}

\begin{frame}{Estimation of a 1-dim Gaussian mean and variance (cont.)}
  \vspace{-5pt}
%  \only<1>{
%    \begin{center}
%      \includegraphics[scale=0.7]{figure/vbupdate}
%    \end{center}
%  } 
  \only<1>{
    \begin{center}
      \includegraphics[scale=0.7]{figure/vbupdate_7}
    \end{center}
  } 
  \only<2>{
    \begin{center}
      \includegraphics[scale=0.7]{figure/vbupdate_1}
    \end{center}
  } 
  \only<3>{
    \begin{center}
      \includegraphics[scale=0.7]{figure/vbupdate_2}
    \end{center}
  } 
  \only<4>{
    \begin{center}
      \includegraphics[scale=0.7]{figure/vbupdate_3}
    \end{center}
  } 
  \only<5>{
    \begin{center}
      \includegraphics[scale=0.7]{figure/vbupdate_4}
    \end{center}
  } 
\end{frame}








